{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cdb339-e656-45e7-9bac-c551a8e94302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to: cleaned_dataset2.csv\n",
      "Final shape: (0, 8)\n",
      "Now, ALL rows have a unique non-null submission_id, valid timestamp, and required features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "\n",
    "# -------- Pipeline Settings --------\n",
    "\n",
    "REQUIRED_FEATURES = [\n",
    "    \"prompt\",\n",
    "    \"submission_text\",\n",
    "    \"submission_type\",\n",
    "    \"subject_domain\",\n",
    "    \"token_count\",\n",
    "    \"origin_label\",\n",
    "    \"timestamp\",\n",
    "    \"submission_id\"\n",
    "]\n",
    "\n",
    "VALID_ORIGIN_LABELS = [\"human\", \"ai\"]\n",
    "\n",
    "# -------- Helper Functions --------\n",
    "\n",
    "def update_timestamps(df):\n",
    "    \"\"\"Fill missing/invalid timestamp entries with current processing time (ISO string).\"\"\"\n",
    "    now = datetime.now().isoformat()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    mask = df[\"timestamp\"].isna()\n",
    "    df.loc[mask, \"timestamp\"] = pd.to_datetime(now)\n",
    "    # Convert all to ISO strings for export consistency\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].replace({np.nan: None, \"NaT\": None})\n",
    "    return df\n",
    "\n",
    "def assign_unique_ids(df):\n",
    "    \"\"\"Ensure every row has a unique string submission_id, filling missing or duplicates\n",
    "    with sequencial values \"1\", \"2\", etc.\"\"\"\n",
    "    # Find highest integer in existing IDs (if any)\n",
    "    last_id = df[\"submission_id\"].apply(\n",
    "        lambda x: int(x.strip() or \"0\") if (isinstance(x, str) and x.strip().isdigit())\n",
    "        else (int(str(x or \"0\")) if (str(x or \"\").strip().isdigit()) else 0)\n",
    "    ).max()\n",
    "    new_id = max(0, last_id) + 1  # Start with next available integer\n",
    "\n",
    "    # Identify rows where submission_id is missing, empty, or duplicate\n",
    "    mask = df[\"submission_id\"].isna() | (df[\"submission_id\"] == \"\") | df.duplicated(subset=\"submission_id\", keep=False)\n",
    "    # Assign new IDs to those rows\n",
    "    for idx in df[mask].index:\n",
    "        df.at[idx, \"submission_id\"] = str(new_id)\n",
    "        new_id += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and standardize the dataset as per GenAI requirements.\"\"\"\n",
    "    # Add missing required columns\n",
    "    for col in REQUIRED_FEATURES:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    # Keep only required columns, in order\n",
    "    df = df[REQUIRED_FEATURES]\n",
    "\n",
    "    # Assign unique IDs to missing/duplicate rows\n",
    "    df = assign_unique_ids(df)\n",
    "\n",
    "    # Fill missing submission_type and subject_domain\n",
    "    df[\"submission_type\"] = df[\"submission_type\"].fillna(\"unknown\")\n",
    "    df[\"subject_domain\"] = df[\"subject_domain\"].fillna(\"unknown\")\n",
    "\n",
    "    # Fill missing token_count from submission_text word count\n",
    "    df[\"token_count\"] = df.apply(\n",
    "        lambda row: len(str(row[\"submission_text\"]).split())\n",
    "        if pd.isnull(row[\"token_count\"]) and pd.notnull(row[\"submission_text\"])\n",
    "        else row[\"token_count\"],\n",
    "        axis=1\n",
    "    )\n",
    "    df[\"token_count\"] = pd.to_numeric(df[\"token_count\"], errors=\"coerce\")\n",
    "\n",
    "    # Validate and standardize origin_label\n",
    "    df[\"origin_label\"] = df[\"origin_label\"].str.lower().str.strip()\n",
    "    df.loc[~df[\"origin_label\"].isin(VALID_ORIGIN_LABELS), \"origin_label\"] = None\n",
    "\n",
    "    # Fill missing/invalid timestamps with current time\n",
    "    df = update_timestamps(df)\n",
    "\n",
    "    # Drop rows missing critical features (except submission_id, now guaranteed unique)\n",
    "    critical = [\"submission_text\", \"origin_label\", \"timestamp\"]\n",
    "    df = df.dropna(subset=critical)\n",
    "\n",
    "    return df\n",
    "\n",
    "def balance_data(df):\n",
    "    \"\"\"Balance dataset so that 'human' and 'ai' have equal representation.\"\"\"\n",
    "    label_counts = df[\"origin_label\"].value_counts()\n",
    "    if set(VALID_ORIGIN_LABELS).issubset(label_counts.index):\n",
    "        min_size = label_counts.min()\n",
    "        balanced = []\n",
    "        for label in VALID_ORIGIN_LABELS:\n",
    "            balanced.append(df[df[\"origin_label\"] == label].sample(min_size, random_state=42))\n",
    "        balanced_df = pd.concat(balanced, ignore_index=True)\n",
    "        return balanced_df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def save_data(df, output_path):\n",
    "    \"\"\"Save DataFrame to CSV, XLSX, or JSON.\"\"\"\n",
    "    if output_path.endswith('.csv'):\n",
    "        df.to_csv(output_path, index=False)\n",
    "    elif output_path.endswith('.xlsx'):\n",
    "        df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "    elif output_path.endswith('.json'):\n",
    "        df.to_json(output_path, orient=\"records\", indent=2)\n",
    "    else:\n",
    "        raise ValueError(\"Output file format not supported. Use CSV, XLSX, or JSON.\")\n",
    "\n",
    "def load_data(input_path):\n",
    "    \"\"\"Load data from CSV, XLSX, or JSON.\"\"\"\n",
    "    if input_path.endswith('.csv'):\n",
    "        df = pd.read_csv(input_path)\n",
    "    elif input_path.endswith('.xlsx'):\n",
    "        df = pd.read_excel(input_path, engine='openpyxl')\n",
    "    elif input_path.endswith('.json'):\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"File format not supported. Use CSV, XLSX, or JSON.\")\n",
    "    return df\n",
    "\n",
    "# -------- Main Pipeline --------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- USER: Set these paths and flags ---\n",
    "    input_path = \"Documents/test.xlsx\"    # or .xlsx, .json\n",
    "    output_path = \"cleaned_dataset2.csv\" # or .xlsx, .json\n",
    "    balance = True                      # Set to False to skip balancing\n",
    "\n",
    "    # Load, clean, (optionally) balance, save\n",
    "    df = load_data(input_path)\n",
    "    df = clean_data(df)\n",
    "    if balance:\n",
    "        df = balance_data(df)\n",
    "    save_data(df, output_path)\n",
    "\n",
    "    print(f\"Cleaned data written to: {output_path}\")\n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    print(\"Now, ALL rows have a unique non-null submission_id, valid timestamp, and required features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf2b90-67cb-42bc-a68d-162d2fcc7b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.anaconda]",
   "language": "python",
   "name": "conda-env-.anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
