{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6f7f8",
   "metadata": {},
   "source": [
    "# AI-content Collector \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108dd39",
   "metadata": {},
   "source": [
    "This notebook was created to automate the process of generating AI responses using a predefined prompt. All of the responses would be compiled to form a part of the unified dataset. The two core models implemented in this program were ChatGPT and Gemini. Other models would be tested and configured in the later phase of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8876224",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef395eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "import config  # We'll keep this temporarily for backward compatibility\n",
    "from  abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a1262",
   "metadata": {},
   "source": [
    "## Base collector class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529710d1",
   "metadata": {},
   "source": [
    "This is the base class for all data collectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b331c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AICollector(ABC):\n",
    "    def __init__(self, client ,delay: float = 1.0):\n",
    "        self.client = client\n",
    "        self.delay = delay\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def api_call(self, prompt) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def format_record(self, record: Dict) -> Dict[str, Any]:\n",
    "        \"Trasform the responses into the standard format\"\n",
    "        pass\n",
    "\n",
    "    def save_to_csv(self, filename ,records:List[Dict[str, Any]], mode: str):\n",
    "        \"\"\"\n",
    "        Save records incrementally to a CSV file.\n",
    "        Handles nested directory creation.\n",
    "        \"\"\"\n",
    "        fieldnames = [\"ID\", \"Text\", \"OriginLabel\", \"SubmissionType\", \"Domain\", \"Timestamp\", \"PromptID\", \"GeneratedModel\" ]\n",
    "\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        filepath = os.path.join('data', filename)\n",
    "\n",
    "        if mode == 'w' or not os.path.exists(filepath):   \n",
    "            with open(filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "                \n",
    "                writer = csv.DictWriter(file, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "                if records:\n",
    "                    writer.writerows(records)\n",
    "\n",
    "        else:\n",
    "            with open(filepath, 'a', newline = '', encoding = 'utf-8') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "                if records:\n",
    "                    writer.writerows(records)\n",
    "            \n",
    "        # Add this method to your AICollector base class\n",
    "    \n",
    "    def clean_json_content(self, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the generated content by removing markdown code block syntax.\n",
    "        \"\"\"\n",
    "        # Remove markdown code block markers\n",
    "        \n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]  # Remove '```json'\n",
    "        elif content.startswith('```'):\n",
    "            content = content[3:]   # Remove '```'\n",
    "        \n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]  # Remove closing '```'\n",
    "        \n",
    "        # Strip any leading/trailing whitespace and newlines\n",
    "        content = content.strip()\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def save_to_json(self, filename, records: Any, mode: str):\n",
    "        \"\"\"Save the responses incrementally to a JSON file.\"\"\" \n",
    "        os.makedirs('newdata', exist_ok=True)\n",
    "        filepath = os.path.join('newdata', filename)\n",
    "        \n",
    "        # Clean the content if it's a string (AI response)\n",
    "        if isinstance(records, str):\n",
    "            cleaned_content = self.clean_json_content(records)\n",
    "            try:\n",
    "                # Parse the cleaned content to ensure it's valid JSON\n",
    "                records = json.loads(cleaned_content)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON for {filename}: {e}\")\n",
    "                # Save as raw text for debugging\n",
    "                with open(filepath.replace('.json', '_raw.txt'), 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_content)\n",
    "                return\n",
    "    \n",
    "        if mode == 'w' or not os.path.exists(filepath):\n",
    "            with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                json.dump(records, file, indent=4, ensure_ascii=False)\n",
    "        else:\n",
    "            with open(filepath, 'a', encoding='utf-8') as file:\n",
    "                json.dump(records, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    def batch_generate(self, promptfile ,datapath: str):\n",
    "        \"\"\"\n",
    "        Generate responses in batches to avoid rate limits.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(promptfile, 'r') as f:\n",
    "                prompt_template = f.read()  \n",
    "            \n",
    "            for filename in os.listdir(datapath):\n",
    "                filepath = os.path.join(datapath, filename)  \n",
    "                \n",
    "                if not filename.endswith('.json'):\n",
    "                    continue\n",
    "                    \n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    file_data = json.load(f)\n",
    "                \n",
    "                current_prompt = prompt_template.replace('[file_to_write]', json.dumps(file_data))\n",
    "                \n",
    "                response = self.api_call(current_prompt)\n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "                if not response:\n",
    "                    print(f\"No response for file: {filename}\")\n",
    "                    continue\n",
    "                    \n",
    "                record = self.format_record(response)\n",
    "                filename = os.path.splitext(filename)[0]\n",
    "                new_filename = f\"{filename}_final.json\"\n",
    "                self.save_to_json(new_filename, record, 'w')\n",
    "                print(f\"Processed file: {filename}, saved as: {new_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error in batch processing: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e88aae",
   "metadata": {},
   "source": [
    "## ChatGPT Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e552b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class ChatGPTCollector(AICollector):\n",
    "    def __init__(self, delay: float = 1.0):\n",
    "        \"\"\" Initialize the ChatGPTCollector with a base schema and delay.\"\"\"\n",
    "        # Get API key from environment variable with fallback to config\n",
    "        api_key = os.environ.get(\"OPENAI_API_KEY\") or config.OPENAI_API_KEY\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        super().__init__(self.client,  delay)\n",
    "        \n",
    "    \n",
    "    def api_call(self, prompt):\n",
    "        \"\"\"\n",
    "        Call the OpenAI API with the provided prompt.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model = \"gpt-4o\",\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call: {e}\")\n",
    "            return str(e)\n",
    "        \n",
    "    def format_record(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        Transform the response into the standard format.\n",
    "        Returns the raw content string for further processing.\n",
    "        \"\"\"\n",
    "        return response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7973fc",
   "metadata": {},
   "source": [
    "## GeminiCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0a6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiCollector(AICollector):\n",
    "    def __init__(self, delay: float = 1.0):\n",
    "        \"\"\" Initialize the GeminiCollector with a base schema and delay.\"\"\"\n",
    "        # Get API key from environment variable with fallback to config\n",
    "        api_key = os.environ.get(\"GEMINI_API_KEY\") or config.GEMINI_API_KEY\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        super().__init__(self.client, delay)\n",
    "    \n",
    "    def api_call(self, prompt):\n",
    "        \"\"\"\n",
    "        Call the OpenAI API with the provided prompt.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro\", contents=prompt\n",
    "        )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call: {e}\")\n",
    "            return ''\n",
    "            \n",
    "    def format_record(self, response: str) -> str:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc2e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(file_path: str, base_schema_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates a JSON file against the base schema structure.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file to validate\n",
    "        base_schema_path: Path to the base schema JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results including:\n",
    "        - is_valid: Boolean indicating if the schema is valid\n",
    "        - errors: List of validation error messages\n",
    "    \"\"\"\n",
    "    # Load the files\n",
    "    try:\n",
    "        with open(base_schema_path, 'r', encoding='utf-8') as f:\n",
    "            base_schema = json.load(f)\n",
    "    except Exception as e:\n",
    "        return {\"is_valid\": False, \"errors\": [f\"Failed to load base schema: {str(e)}\"]}\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            test_schema = json.load(f)\n",
    "    except Exception as e:\n",
    "        return {\"is_valid\": False, \"errors\": [f\"Failed to load test file: {str(e)}\"]}\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {\"is_valid\": True, \"errors\": []}\n",
    "    \n",
    "    # Check required top-level keys\n",
    "    required_top_keys = {\"domain\", \"prompt\", \"rubric\", \"submissions\"}\n",
    "    missing_keys = required_top_keys - set(test_schema.keys())\n",
    "    if missing_keys:\n",
    "        results[\"is_valid\"] = False\n",
    "        results[\"errors\"].append(f\"Missing required top-level keys: {', '.join(missing_keys)}\")\n",
    "    \n",
    "    # Check rubric structure if it exists\n",
    "    if \"rubric\" in test_schema:\n",
    "        if not isinstance(test_schema[\"rubric\"], dict):\n",
    "            results[\"is_valid\"] = False\n",
    "            results[\"errors\"].append(\"'rubric' must be a dictionary\")\n",
    "        else:\n",
    "            # Check for rubric_id\n",
    "            if \"rubric_id\" not in test_schema[\"rubric\"]:\n",
    "                results[\"is_valid\"] = False\n",
    "                results[\"errors\"].append(\"Missing 'rubric_id' in rubric\")\n",
    "                \n",
    "            # Check criteria\n",
    "            if \"criteria\" not in test_schema[\"rubric\"]:\n",
    "                results[\"is_valid\"] = False\n",
    "                results[\"errors\"].append(\"Missing 'criteria' in rubric\")\n",
    "            elif not isinstance(test_schema[\"rubric\"][\"criteria\"], list):\n",
    "                results[\"is_valid\"] = False\n",
    "                results[\"errors\"].append(\"'criteria' must be a list\")\n",
    "            else:\n",
    "                # Check each criterion\n",
    "                for i, criterion in enumerate(test_schema[\"rubric\"][\"criteria\"]):\n",
    "                    if not isinstance(criterion, dict):\n",
    "                        results[\"is_valid\"] = False\n",
    "                        results[\"errors\"].append(f\"Criterion {i} must be a dictionary\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Check required criterion keys\n",
    "                    criterion_keys = {\"criterion_id\", \"name\", \"description\", \"performance_descriptors\"}\n",
    "                    missing_criterion_keys = criterion_keys - set(criterion.keys())\n",
    "                    if missing_criterion_keys:\n",
    "                        results[\"is_valid\"] = False\n",
    "                        results[\"errors\"].append(f\"Criterion {i} is missing keys: {', '.join(missing_criterion_keys)}\")\n",
    "                    \n",
    "                    # Check performance_descriptors structure\n",
    "                    if \"performance_descriptors\" in criterion:\n",
    "                        if not isinstance(criterion[\"performance_descriptors\"], dict):\n",
    "                            results[\"is_valid\"] = False\n",
    "                            results[\"errors\"].append(f\"'performance_descriptors' in criterion {i} must be a dictionary\")\n",
    "                        elif len(criterion[\"performance_descriptors\"]) == 0:\n",
    "                            results[\"is_valid\"] = False\n",
    "                            results[\"errors\"].append(f\"'performance_descriptors' in criterion {i} cannot be empty\")\n",
    "                        # We don't validate specific performance descriptor keys as they can vary\n",
    "    \n",
    "    # Check submissions structure if it exists\n",
    "    if \"submissions\" in test_schema:\n",
    "        if not isinstance(test_schema[\"submissions\"], list):\n",
    "            results[\"is_valid\"] = False\n",
    "            results[\"errors\"].append(\"'submissions' must be a list\")\n",
    "        else:\n",
    "            # Check each submission\n",
    "            for i, submission in enumerate(test_schema[\"submissions\"]):\n",
    "                if not isinstance(submission, dict):\n",
    "                    results[\"is_valid\"] = False\n",
    "                    results[\"errors\"].append(f\"Submission {i} must be a dictionary\")\n",
    "                    continue\n",
    "                \n",
    "                # Check required submission keys\n",
    "                submission_keys = {\"quality\", \"llm_questions\", \"llm_answers\", \"final_submission\", \"feedback\"}\n",
    "                missing_submission_keys = submission_keys - set(submission.keys())\n",
    "                if missing_submission_keys:\n",
    "                    results[\"is_valid\"] = False\n",
    "                    results[\"errors\"].append(f\"Submission {i} is missing keys: {', '.join(missing_submission_keys)}\")\n",
    "                \n",
    "                # Check llm_questions and llm_answers are lists with matching lengths\n",
    "                if \"llm_questions\" in submission and \"llm_answers\" in submission:\n",
    "                    if not isinstance(submission[\"llm_questions\"], list):\n",
    "                        results[\"is_valid\"] = False\n",
    "                        results[\"errors\"].append(f\"'llm_questions' in submission {i} must be a list\")\n",
    "                    if not isinstance(submission[\"llm_answers\"], list):\n",
    "                        results[\"is_valid\"] = False\n",
    "                        results[\"errors\"].append(f\"'llm_answers' in submission {i} must be a list\")\n",
    "                    if (isinstance(submission[\"llm_questions\"], list) and \n",
    "                        isinstance(submission[\"llm_answers\"], list) and \n",
    "                        len(submission[\"llm_questions\"]) != len(submission[\"llm_answers\"])):\n",
    "                        results[\"is_valid\"] = False\n",
    "                        results[\"errors\"].append(f\"'llm_questions' and 'llm_answers' in submission {i} must have the same length\")\n",
    "                \n",
    "                # Check feedback structure\n",
    "                if \"feedback\" in submission:\n",
    "                    if not isinstance(submission[\"feedback\"], dict):\n",
    "                        results[\"is_valid\"] = False\n",
    "                        results[\"errors\"].append(f\"'feedback' in submission {i} must be a dictionary\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def validate_directory(directory_path: str, base_schema_path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validates all JSON files in a directory against the base schema.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing JSON files to validate\n",
    "        base_schema_path: Path to the base schema JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping file names to their validation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Validate each JSON file in the directory\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            results[file_name] = validate_schema(file_path, base_schema_path)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790dcf8",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabaa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the ChatGPTCollector to generate responses\n",
    "#gptcollector = ChatGPTCollector()\n",
    "\n",
    "#gptcollector.batch_generate('prompt.txt', 'data')\n",
    "\n",
    "\n",
    "#Use the GeminiCollector to generate responses\n",
    "gemini_collector = GeminiCollector()\n",
    "\n",
    "gemini_collector.batch_generate('prompt.txt', 'data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f1018d",
   "metadata": {},
   "source": [
    "### Validate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e0814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineer_rubric13_final.json: ✓\n",
      "engineer_rubric16_final.json: ✓\n",
      "engineer_rubric11_final.json: ✓\n",
      "engineer_rubric14_final.json: ✓\n",
      "engineer_rubric19_final.json: ✓\n",
      "engineer_rubric20_final.json: ✓\n",
      "engineer_rubric17_final.json: ✓\n",
      "engineer_rubric12_final.json: ✓\n",
      "engineer_rubric15_final.json: ✓\n",
      "engineer_rubric18_final.json: ✓\n"
     ]
    }
   ],
   "source": [
    "# Directory validation\n",
    "dir_results = validate_directory(\"newdata\", 'sample_dataset.json')\n",
    "for file_name, result in dir_results.items():\n",
    "    print(f\"{file_name}: {'✓' if result['is_valid'] else '✗'}\")\n",
    "    if not result['is_valid']:\n",
    "        for error in result['errors']:\n",
    "            print(f\"  - {error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
